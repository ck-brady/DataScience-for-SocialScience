{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c864b70",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <div align=\"center\"> SPECIAL TOPICS III </div>\n",
    "## <div align=\"center\"> Data Science for Social Scientists  </div>\n",
    "### <div align=\"center\"> ECO 4199 </div>\n",
    "#### <div align=\"center\">Class 11 - Social Biases and Prediction</div>\n",
    "<div align=\"center\"> Fabien Forge, (he/him)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70eb488",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Final Exam\n",
    "- The final exam will take place on: \n",
    "    - __12/16/2021, Thursday__\n",
    "    - __9:30AM - 12:30PM__ (no extension possible!)\n",
    "    - __Brightspace__\n",
    "- The exam will take the same format as the midterm:\n",
    "    - True and False Questions or Multiple Choice Questions\n",
    "- __CAREFUL__: You will be limited in time which means that while this will be an open book exam, you will probably not have enough time to look up all the answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b54709",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final Exam Continued\n",
    "- This is a __cumulative exam__\n",
    "- No code will be required but I may ask questions in which coding is part of the question/hint\n",
    "    - e.g. recall the df.shape part of the question in the midterm\n",
    "- Today's class will be part of the exam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a573c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning\n",
    "- What does it mean to do machine learning?\n",
    "- With the exception of unsupervised learning, all our ML tasks evolved around the same idea:\n",
    "    - Produce predictions of y from x\n",
    "- ML is the art of finding patterns in the data, that can be approximated by some functions\n",
    "- If the function is good enough then its predictions will be close to the observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7df5dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learing's Hype\n",
    "- Why is ML working? \n",
    "- ML fits complex and very flexible functional forms to the data:\n",
    "    - without simply overfitting \n",
    "    - it finds functions that work well out-of-sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0db260",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML vs Classical Econometrics\n",
    "- You can think about econometrics in terms of causal inference (as you should)\n",
    "- Mathematically, you can also think of it in terms of:\n",
    "    1. Find the functional form: interaction term, non-linear term etc\n",
    "    2. Find _estimates_ of the $\\beta$ in the relationship between Y and X\n",
    "- OLS regressions try to estimate $\\hat{\\beta}$. How different is it from:\n",
    "    - $\\hat{\\beta}$ obtained in a LASSO regression?\n",
    "    - $w$ weights from deep learning?\n",
    "- Of course the difference is clearer for non parametric methods such as tree-methods "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037b14d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML vs Classical Econometrics, interpretation\n",
    "- Recall the equation that started each lecture:\n",
    "    - $y = f(x) + \\varepsilon$\n",
    "- One reason to use such representation was to make clear that we were trying to find such function\n",
    "- But of course, OLS is also a function that related X to y\n",
    "- In econometrics, this formulation is hardly ever used because the important part is $\\beta$\n",
    "- Thus, ML is a fantastic tool to obtain $y \\approx \\hat{y} = f(x)$ while causal inference is a way of finding $\\beta \\approx \\mathbb{E}[\\hat{\\beta}|x]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab415ec7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML vs Classical Econometrics, interpretation continued\n",
    "\n",
    "- This means that machine learning cannot be used for causal inference\n",
    "- Even if the model used is linear and the functional form resemble:\n",
    "    - $f(x) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p + \\varepsilon$\n",
    "- Because the $\\hat{\\beta}_{ML}$ obtained in ML do not have the same properties as the ones from causal inference $\\hat{\\beta}_{OLS}$\n",
    "- Indeed $\\hat{\\beta}_{ML}$ are the ones that make $\\hat{y}$ close to $y$ __not__ close to $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045db4aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Most of Machine Learning in One Expression\n",
    "$\\underbrace{\\min \\sum_{i=1}^n L(f(x_i), y_i)}_{\\text{in-sample loss}} \\text{ over } \\underbrace{f \\in F}_{\\text{function class}} \\text{ subject to } \\underbrace{R(f) \\leq c}_{\\text{complexity restriction}}$\n",
    "- All (most) ML tasks try to minimize an in-sample loss function $L(.)$\n",
    "    - We mainly say mean squared error (or RSS) but others exist\n",
    "    - For instance, for classification, false positive may have different weights from false negative\n",
    "- This is done using a function class:\n",
    "    - OLS, LASSO, Random Forest etc.\n",
    "- And using penalties to make sure that function's complexity does not lead to overfitting\n",
    "    - Hyper-parameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7389391",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Most of Machine Learning in One Expression, continued\n",
    "- __Global/parametric predictors__:\n",
    "    - LASSO = $\\|\\beta\\|_1 = \\sum_{j=1}^k \\beta_j $\n",
    "    - Ridge = $\\|\\beta\\|_2 = \\sum_{j=1}^k \\beta_j^2 $\n",
    "- __Local/nonparametric predictors__:\n",
    "    - Decision/regression trees = Depth, number of nodes/leaves, minimal leaf size\n",
    "    - Random forest (linear combination of trees) = Number of trees, number of variables used in each tree, size of bootstrap sample, complexity of trees\n",
    "    - Nearest neighbors = Number of neighbors\n",
    "- __Mixed predictors__:\n",
    "    - Deep learning =  Number of levels (depth), number of neurons per level (width), connectivity between neurons (density)\n",
    "- __Combined predictors__:\n",
    "    - Bagging: Number of draws, size of bootstrap samples (and individual regularization parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4967ddd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Function Classes\n",
    "- Recall that there is no one function class that is better over all predictive tasks all the time\n",
    "- If you want to know which performs best a good way would to try them all at once\n",
    "- It turns out that [PyCaret](https://pycaret.org/) can do this for you\n",
    "- We will follow their [advanced tutorial](https://github.com/pycaret/pycaret/blob/master/tutorials/Regression%20Tutorial%20Level%20Intermediate%20-%20REG102.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a97b47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pycart Set up\n",
    "- The setup() function initializes the environment in pycaret and creates the transformation pipeline to prepare the data for modeling . setup() must be called before executing any other function in pycaret. \n",
    "- It takes two mandatory parameters: a pandas dataframe and the name of the target column. \n",
    "- All other parameters are optional and are used to customize the pre-processing pipeline (we will see them in later tutorials)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacfe376",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Biases and Machine Learning\n",
    "- There are two sources of biases that we will explore\n",
    "- A first source of bias will be linked to the data used\n",
    "- The second is linked to what the complexity restriction implies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e826c6d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML and data-induced biases\n",
    "- Deep learning can be used to predict what is represented on a picture\n",
    "- It can also be used to enhance images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaa3f5f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://4.img-dpreview.com/files/p/E~TS590x0~articles/4871415337/googlebrain.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c2b3f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML and Upsampling\n",
    "- The way to do this is beyond the scope of this lecture\n",
    "- But the method isn't very different from what we have learned\n",
    "- The upsampling in the previous example takes a matrix X of dimension 8x8\n",
    "- It outputs a matrix Y of dimension 32x32\n",
    "$$\\mathbf{Y} = f(\\mathbf{X})$$\n",
    "- For every pixel, the function output 4 pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f5ffd0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML training\n",
    "- So a deep learning neural network was trained to predict higher resolution picture\n",
    "- The way to do this is of course to feed the NN a low resolution picture and use the high resolution picture as a target\n",
    "- The NN learns from this data the pixels it should output given a low resolution picture\n",
    "- How good this network is determined by how close it gets to the data it received"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b47d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Who is this guy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c3553",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](obama.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f83fb1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML test\n",
    "- As should be clear by now, ML algorithms are only as good as their prediction out of sample\n",
    "- Seeing the picture from the previous slide it is obvious to us that this low resolution picture represents Obama\n",
    "- We are therefore able to reconstruct the image based on our recollection of Obama's features\n",
    "- Not the trained NN, it is only able to output a new matrix Y, based on the weights that best fitted the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c244fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://cdn.vox-cdn.com/thumbor/MXX-mZqWLQZW8Fdx1ilcFEHR8Wk=/55x85:768x536/1820x1213/filters:focal(336x236:464x364):format(webp)/cdn.vox-cdn.com/uploads/chorus_image/image/66972412/face_depixelizer_obama.0.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87782e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML and out of sample prediction\n",
    "- Did the algorithm fail?\n",
    "- If you were presented with the right picture only and knew this was generated by a computer you would probably be very impressed\n",
    "- When compared to the left picture this is outrageously wrong\n",
    "- But is it enough to talk about biases?\n",
    "- Here are a few other out of sample predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777a09d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://pbs.twimg.com/media/Ea-8T2NXkAEfH6y?format=png&name=900x900)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a28b61",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://pbs.twimg.com/media/Ea_AGceXYAYg4KT?format=jpg&name=medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6424c09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sample induced bias\n",
    "- What is wrong here?\n",
    "- To be clear there is no reason to believe that the people who trained this NN were racists (i.e. they did not add something to their algorithm to make pictures whiter)\n",
    "- Instead the issue is linked the pictures used for training and test    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa12bfc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sample induced bias, continued\n",
    "- Think about your assignment: I first asked you to split your data between train and test\n",
    "- Your models performed more or less well on these datasets\n",
    "- Now if your data is indeed representative of the prediction task at hand this is perfect\n",
    "- But if somehow your dataset was not representative then your out-of-sample MSE would be less meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44276235",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objective function\n",
    "- Another example is Amazon, that used ML to select resumes\n",
    "- The goal was to select the best resumes based on how close they were to employees they already hired\n",
    "- They ended up with predictions that best fit would most of the time be middle aged white males..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38e6c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Complexity restrictions and Biases\n",
    "- Recall that parameters of ML do not have the same interpreation as OLS\n",
    "- One way to think about this is to think about omitted variable bias\n",
    "- If you have OVB and you do not control for the missing variable, then your parameter of interest will capture the effect of the variable in the regression __and__ the variable missing\n",
    "- In the context of complexity restriction you are very likely to restrict parameters that are indeed correlated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d005e2b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "- Say that you want to predict some y based on education and race\n",
    "- It is a statistical fact that, in the US, the black community is __on average__ less educated than the white community\n",
    "- In the context of LASSO, where I do not want too many predictors, perhaps race is slightly less informative and will be shrunk to zero.\n",
    "- You may even want specifically to avoid including race, depending on what your target y is, to make sure you are not including racial biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e898e49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example continued\n",
    "- Many states in the US are now using Machine Learning to predict how a defendantâ€™s risk of future crime\n",
    "- The goal is to remove the judge bias and try to predict \"objectively\" based on some data who was likely to comit a crime again in the future\n",
    "- A classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ea8d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example continued\n",
    "- When performing classification tasks, one can use a confusion matrix\n",
    "\n",
    "|Prediction/Reality| FALSE | TRUE |\n",
    "| ---| --- | --- |\n",
    "|__FALSE__| True Negative | False Negative | \n",
    "|__TRUE__| False Positive | True Positive | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c1b0ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example continued\n",
    "- An important [research](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) looked at the algorithm created by the for-profit company: Northpointe.\n",
    "    - The formula was particularly likely to falsely flag black defendants as future criminals, wrongly labeling them this way at almost twice the rate as white defendants.\n",
    "    - White defendants were mislabeled as low risk more often than black defendants.\n",
    "    - The algorithm was somewhat more accurate than a coin flip. Of those deemed likely to re-offend, 61 percent were arrested for any subsequent crimes within two years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567853e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias explained\n",
    "- Say that you want to predict whether a convict will commit another crime in the future\n",
    "- You are only allowed to use a decision tree\n",
    "- Your tree can only have one split\n",
    "- If it is in your dataset, there's a good chance that race would be the split that gives you the best prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49d9f48",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias explained continued\n",
    "- Think back on our principal component analysis\n",
    "- You don't need a race dummy to caracterized a person of color\n",
    "- You could instead use things like education, the neighborhood in which the person was raised and so on\n",
    "- If these things are sufficiently correlated with race then you would still predict future crime based race __even if__ race is never in your algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd00dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning and decision making\n",
    "- What does it mean for ML as a way to make decision?\n",
    "- Thus, part of Northpointe bias was comming from features\n",
    "- Would it still be a bias if POC were indeed more likely to commit crime and their algorithm had just as many false positive for blacks as they do for white?\n",
    "- This enters a philosophical debate but here is what can be said:\n",
    "    - Machines are learning from data\n",
    "    - Removing the human from the learning process doesn't remove the bias from the data itself\n",
    "- Using ML for making decision requires tools that are not statistical but for which social scientists are well equipped"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
